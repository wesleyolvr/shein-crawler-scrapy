# Crawler Shein

# Bem-vindo ao projeto de Web Scraping automatizado do site Shein!

O projeto automatiza a obtenÃ§Ã£o de informaÃ§Ãµes sobre produtos do site Shein.

## Funcionalidades

- **Web Scraping Automatizado com Scrapy:** Utiliza o framework Scrapy para extrair dados de produtos do site Shein. Os dados sÃ£o recebidos via Apache Kafka, onde sÃ£o processados e retornados como JSON para a API. ğŸŒğŸ“¦
- **Armazenamento em Banco de Dados e Cache Redis:** Utiliza um banco de dados SQLite para armazenar os dados extraÃ­dos e um cache Redis para armazenar informaÃ§Ãµes sobre a Ãºltima extraÃ§Ã£o de cada categoria. Os dados sÃ£o atualizados no banco de dados e no cache Redis somente se houver diferenÃ§a no preÃ§o do produto. ğŸ—ƒï¸ğŸ”
- **ValidaÃ§Ã£o de Dados com Pydantic:** Utiliza o Pydantic para validar os dados extraÃ­dos antes de armazenÃ¡-los no banco de dados. âš™ï¸ğŸ”
- **API FastAPI:** Disponibiliza uma API utilizando o FastAPI para gerenciar os endpoints da aplicaÃ§Ã£o. A API recebe os dados dos produtos via Kafka, valida e verifica se o produto estÃ¡ presente no cache Redis antes de atualizar o banco de dados e o cache. ğŸš€ğŸ”Œ
- **Apache Kafka:** Utiliza o Apache Kafka para permitir a comunicaÃ§Ã£o assÃ­ncrona e distribuÃ­da entre os mÃ³dulos do projeto, como o crawler, a API e o banco de dados. ğŸ“¡ğŸ”—




## InstalaÃ§Ã£o

Para executar este projeto em sua mÃ¡quina local, siga os passos abaixo:

1. **Clone o repositÃ³rio**:
   ```sh
   git clone https://github.com/wesleyolvr/shein-crawler-scrapy.git
   ```

2. **Crie e Ative um Ambiente Virtual**:
   - No terminal, navegue atÃ© o diretÃ³rio do seu projeto:
     ```sh
     cd /path/to/your/project
     ```
   - Crie um ambiente virtual:
     ```sh
     python -m venv venv
     ```
   - Ative o ambiente virtual:
     - No Windows:
     ```sh
       venv\Scripts\activate
     ```
     - No macOS/Linux:
     ```sh
       source venv/bin/activate
     ```

3. **Ajuste os arquivos de variaveis de ambiente**:
   Renomeie o arquivo `config_sample.ini` para `config.ini` e insira as informaÃ§Ãµes do banco de dados, Redis e do Kafka conforme necessÃ¡rio.


4. **Cria e inicie os serviÃ§os Kafka, Zookeeper, Redis, KafDrop, PgAdmin**:
   ```sh
   docker-compose up -d
   ```

5. **Inicie script consumidor dos produtos**:
   ```sh
   python start_consumidor.py
   ```
6. **Inicie a API FastAPI**:
   ```sh
   uvicorn api.main:app --reload
   ```
7. **Inicie o script do Spider:**
   ```sh
   python shein/run_spider.py
   ```

## Uso

Depois de seguir as etapas de instalaÃ§Ã£o, a API estarÃ¡ disponÃ­vel em `http://localhost:8000` e vocÃª pode acessar a documentaÃ§Ã£o interativa do Swagger em `http://localhost:8000/docs`.

### Endpoints disponÃ­veis:

- **`/produtos`**: Lista todos os produtos extraÃ­dos do site Shein.
- **`/produtos/{product_id}`**: Retorna um produto especÃ­fico pelo ID.

## ContribuiÃ§Ã£o

Se vocÃª deseja contribuir com melhorias para este projeto, siga as diretrizes abaixo:

1. Crie uma nova branch:
   ```sh
   git checkout -b feature-nova-funcionalidade
   ```

2. FaÃ§a suas alteraÃ§Ãµes e commit:
   ```sh
   git commit -am 'Adiciona nova funcionalidade'
   ```

3. Envie para o GitHub:
   ```sh
   git push origin feature-nova-funcionalidade
   ```

4. Crie um novo Pull Request e aguarde a revisÃ£o.


## PrÃ³ximos Passos

- **ServiÃ§o de ComparaÃ§Ã£o de PreÃ§os:** Desenvolver um serviÃ§o que compara os preÃ§os atuais dos produtos com seus histÃ³ricos para identificar oportunidades de compra. ğŸ’°ğŸ”

- **DockerizaÃ§Ã£o do Projeto:** Utilizar Docker para empacotar e distribuir todo o projeto, garantindo portabilidade, consistÃªncia e facilitando a escalabilidade. Isso simplificarÃ¡ a gestÃ£o de dependÃªncias e garantirÃ¡ uma implantaÃ§Ã£o mais eficiente. ğŸ³ğŸš€



## LicenÃ§a

Este projeto Ã© licenciado sob a [MIT License](https://opensource.org/licenses/MIT) - veja o arquivo [LICENSE](https://github.com/seu-usuario/nome-do-projeto/blob/main/LICENSE) para mais detalhes. ğŸš€ğŸ¤